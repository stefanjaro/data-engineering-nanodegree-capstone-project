{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Data Processing\n",
    "\n",
    "The staging script for the Temperature Data Processing Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Job\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark():\n",
    "    \"\"\"\n",
    "    Initializes a spark instance\n",
    "    \"\"\"\n",
    "    # initialize spark\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"temperature-data-processing\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_data(spark, input_fp):\n",
    "    \"\"\"\n",
    "    Loads in the temperature data \n",
    "    and filters out non-US records and nulls\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark session\n",
    "        An initialized spark session\n",
    "    input_fp: str\n",
    "        The location of the temperature data file\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    temp_schema = StructType([\n",
    "        StructField(\"dt\", DateType()),\n",
    "        StructField(\"AverageTemperature\", DoubleType()),\n",
    "        StructField(\"AverageTemperatureUncertainty\", DoubleType()),\n",
    "        StructField(\"City\", StringType()),\n",
    "        StructField(\"Country\", StringType()),\n",
    "        StructField(\"Latitude\", StringType()),\n",
    "        StructField(\"Longitude\", StringType())\n",
    "    ])\n",
    "\n",
    "    temp = spark.read.csv(input_fp, header=True, schema=temp_schema)\n",
    "\n",
    "    # filter out all other countries except for the US\n",
    "    temp = temp.where(temp[\"Country\"] == \"United States\")\n",
    "\n",
    "    # drop all nulls in the average temperature column\n",
    "    temp = temp.dropna(how=\"any\", subset=[\"AverageTemperature\"])\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_state_id(spark, temp, demo_lookup_fp, port_lookup_fp):\n",
    "    \"\"\"\n",
    "    Combines the state city lookup tables created using the\n",
    "    demographic data and airport codes data and maps the state ids\n",
    "    to the cities in the temperature dataset\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark session\n",
    "        An initialized spark session\n",
    "    temp: spark dataframe\n",
    "        The temperature dataset\n",
    "    demo_lookup_fp: str\n",
    "        The location of the lookup table created using the demographic data\n",
    "    port_lookup_fp: str\n",
    "        The location of the lookup table created using the airport codes data\n",
    "    \"\"\"\n",
    "\n",
    "    # import files needed for mapping state codes\n",
    "    state_city_lookup_demo = spark.read.parquet(demo_lookup_fp)\n",
    "    state_city_lookup_port = spark.read.parquet(port_lookup_fp)\n",
    "\n",
    "    # clean up the files before appending them together\n",
    "    state_city_lookup_demo = state_city_lookup_demo.select(\"state_id\", \"City\")\n",
    "    state_city_lookup_demo = state_city_lookup_demo.dropna(how=\"any\")\n",
    "    state_city_lookup_demo = state_city_lookup_demo.where(\n",
    "        (state_city_lookup_demo[\"City\"] != \"None\") &\n",
    "        (state_city_lookup_demo[\"state_id\"] != \"None\")\n",
    "    )\n",
    "\n",
    "    state_city_lookup_port = state_city_lookup_port.withColumnRenamed(\"municipality\", \"City\")\n",
    "    state_city_lookup_port = state_city_lookup_port.dropna(how=\"any\")\n",
    "    state_city_lookup_port = state_city_lookup_port.where(\n",
    "        (state_city_lookup_port[\"City\"] != \"None\") &\n",
    "        (state_city_lookup_port[\"state_id\"] != \"None\")\n",
    "    )\n",
    "\n",
    "    # append the files together and remove duplicate records\n",
    "    state_lookup = state_city_lookup_demo.union(state_city_lookup_port).distinct()\n",
    "\n",
    "    # there are undoubtedly cities with the same name that have been mapped to multiple states\n",
    "    # we can't tell which of those in the temperature dataset it belongs to\n",
    "    # so we're going to do a clean de-duplicate\n",
    "    state_lookup = state_lookup.dropDuplicates([\"City\"])\n",
    "\n",
    "    # merge the state_id into the temperature dataset\n",
    "    # of the 661524 records, only 5407 get dropped\n",
    "    # these are the observations for only 2 cities\n",
    "    temp = temp.join(\n",
    "        F.broadcast(state_lookup),\n",
    "        on=\"City\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_fact(temp, output_fp):\n",
    "    \"\"\"\n",
    "    Create the temperature fact table\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    temp: spark dataframe\n",
    "        The temperature data\n",
    "    output_fp: str\n",
    "        The location to store the fact table\n",
    "    \"\"\"\n",
    "    # extract the year from the dt\n",
    "    temp = temp.withColumn(\"year\", F.year(temp[\"dt\"]))\n",
    "\n",
    "    # create the starter table for the fact table\n",
    "    fact_temp = temp.select(\"state_id\", \"year\").dropDuplicates()\n",
    "\n",
    "    # number of cities\n",
    "    fact_temp = fact_temp.join(\n",
    "        temp.groupBy(\"state_id\", \"year\").agg(F.countDistinct(\"City\")),\n",
    "        on=[\"state_id\", \"year\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # summary statistics on average temperature\n",
    "    for stat in [\"count\", \"mean\", \"max\", \"min\"]:\n",
    "        fact_temp = fact_temp.join(\n",
    "            temp.groupBy(\"state_id\", \"year\").agg({\"AverageTemperature\": stat}),\n",
    "            on=[\"state_id\", \"year\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # rename columns\n",
    "    fact_temp_col_names = {\n",
    "        \"count(City)\": \"num_of_cities\",\n",
    "        \"count(AverageTemperature)\": \"observation_count\",\n",
    "        \"avg(AverageTemperature)\": \"avg_temp\",\n",
    "        \"max(AverageTemperature)\": \"max_temp\",\n",
    "        \"min(AverageTemperature)\": \"min_temp\"\n",
    "    }\n",
    "\n",
    "    for k,v in fact_temp_col_names.items():\n",
    "        fact_temp = fact_temp.withColumnRenamed(k,v)\n",
    "\n",
    "    # calculate medians (approximate, not precise)\n",
    "    fact_temp = fact_temp.join(\n",
    "        temp.groupBy(\"state_id\", \"year\").agg(\n",
    "            F.percentile_approx(\"AverageTemperature\", 0.5).alias(\"median_temp\")\n",
    "        ),\n",
    "        on=[\"state_id\", \"year\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    fact_temp.write.partitionBy(\"year\").parquet(output_fp + \"fact_temperature/\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Runs the spark job\n",
    "    \"\"\"\n",
    "    # hardcoded variables\n",
    "    input_fp = \"../data/GlobalLandTemperaturesByCity.csv\"\n",
    "    output_fp = \"../data/output_files/\"\n",
    "    demo_lookup_fp = \"../data/preprocessed_files/state_city_lookup_demo/\"\n",
    "    port_lookup_fp = \"../data/preprocessed_files/state_city_lookup_ports/\"\n",
    "\n",
    "    # run the spark job\n",
    "    spark = initialize_spark()\n",
    "    temp = load_and_filter_data(spark, input_fp)\n",
    "    temp = map_state_id(spark, temp, demo_lookup_fp, port_lookup_fp)\n",
    "    create_temp_fact(temp, output_fp)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 14:24:10 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.140.190 instead (on interface wlo1)\n",
      "22/04/07 14:24:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/07 14:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run the function above\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/10 00:18:22 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.9 instead (on interface wlo1)\n",
      "22/04/10 00:18:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/10 00:18:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# initialize spark\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load fact table\n",
    "fact_temp = spark.read.parquet(\"../data/output_files/fact_temperature\")\n",
    "\n",
    "# length\n",
    "fact_temp.count()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ae5f699e6c33511a9bd2522a947541f4f974526b22d36b2404541bec39e83fe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
