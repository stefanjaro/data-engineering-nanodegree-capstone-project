{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration Data Processing\n",
    "\n",
    "The staging script for the Immigration Data Processing Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import datetime\n",
    "import configparser\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Spark Job\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark():\n",
    "    \"\"\"\n",
    "    Initializes a spark instance\n",
    "    \"\"\"\n",
    "    # initialize spark\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"immigration-data-preprocessing\")\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list_from_s3(s3_link_prefix, file_prefix):\n",
    "    \"\"\"\n",
    "    Connects to s3, gets the list of files with the prefix stated and\n",
    "    dynamically generates the list of SAS files that to be read in by spark\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    s3_link_prefix: str\n",
    "        The link to the bucket\n",
    "    file_prefix: str\n",
    "        The prefix of the files we're looking for\n",
    "    \"\"\"\n",
    "    # get our aws keys from the locally stored config file\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dl.cfg')\n",
    "    AWS_ACCESS_KEY_ID = config['DEFAULT']['AWS_ACCESS_KEY_ID']\n",
    "    AWS_SECRET_KEY_ID = config['DEFAULT']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "    # create a boto3 s3 instance\n",
    "    s3 = boto3.resource(\n",
    "        \"s3\",\n",
    "        region_name=\"us-west-2\",\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_KEY_ID\n",
    "    )\n",
    "\n",
    "    # connect to our bucket\n",
    "    bucket = s3.Bucket(s3_link_prefix.split(\"/\")[-2].strip())\n",
    "\n",
    "    # get a list of all objects with the file_prefix\n",
    "    all_objects = [obj for obj in bucket.objects.filter(Prefix=file_prefix)]\n",
    "\n",
    "    # create the list of files to be read\n",
    "    list_of_files = []\n",
    "    for obj in all_objects:\n",
    "        list_of_files.append(s3_link_prefix + obj.key)\n",
    "\n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_immigration_and_add_labels(spark, imm_fp, mode_labels_fp, port_labels_fp, \n",
    "                                      visa_labels_fp, cit_labels_fp, cols_required):\n",
    "    \"\"\"\n",
    "    Imports all of the data required for immigration pre-processing\n",
    "    Filters the immigration data down to only the columns we need\n",
    "    And adds all of the labels to the codified values we're interested in\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    imm_fp: str\n",
    "        The location of the immigration data files\n",
    "    mode_labels_fp: str\n",
    "        The location of the labels file for i94mode\n",
    "    port_labels_fp: str\n",
    "        The location of the labels file for i94port\n",
    "    visa_labels_fp: str\n",
    "        The location of the labels file for i94visa\n",
    "    cit_labels_fp: str\n",
    "        The location of the labels file for i94cit\n",
    "    cols_required: list\n",
    "        The list of columns we require from the immigration data file\n",
    "    \"\"\"\n",
    "    # import the sas files\n",
    "    imm = spark.read.format('com.github.saurfang.sas.spark').load(imm_fp)\n",
    "\n",
    "    # import label files\n",
    "    mode_labels = spark.read.csv(mode_labels_fp, header=True) # for mode of travel\n",
    "    port_labels = spark.read.csv(port_labels_fp, header=True) # for port of entry\n",
    "    visa_labels = spark.read.csv(visa_labels_fp, header=True) # for type of visa\n",
    "    citz_labels = spark.read.csv(cit_labels_fp, header=True) # for citizenship country\n",
    "\n",
    "    # select only the columns we need\n",
    "    imm = imm.select(cols_required)\n",
    "\n",
    "    # add labels to codified values\n",
    "    for df in [mode_labels, port_labels, visa_labels, citz_labels]:\n",
    "        imm = imm.join(\n",
    "            F.broadcast(df),\n",
    "            on=df.columns[0],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    return imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_immigration_data(imm, preprocessed_output_fp):\n",
    "    \"\"\"\n",
    "    Proprocess the immigration data by:\n",
    "        1. Removing records with irrelevant and invalid values\n",
    "        2. Removing records without an arrival date\n",
    "        3. Adjusting the format of the arrival date\n",
    "        4. Adjusting the gender column so everything apart from M and F is O for Other\n",
    "        5. Bucketing the ages into distinct categories\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    imm: spark dataframe\n",
    "        The immigration dataset\n",
    "    preprocessed_output_fp: str\n",
    "        The location to save the preprocessed file\n",
    "    \"\"\"\n",
    "    # hardcoded variables\n",
    "    SAS_START_DATE = datetime.datetime(1960, 1, 1)\n",
    "\n",
    "    # remove the irrelevant ports and invalid citizenship countries from the dataset\n",
    "    imm = imm.where(\n",
    "        (imm[\"i94port_relevant\"] == \"RELEVANT\") &\n",
    "        (imm[\"i94cit_continent\"] != \"INVALID\")\n",
    "    )\n",
    "    \n",
    "    # clean up arrival date which is the number of days after January 1, 1960\n",
    "    imm = imm.where(imm[\"arrdate\"].isNotNull())    \n",
    "    adjust_date = F.udf(lambda x: SAS_START_DATE + datetime.timedelta(days=x), DateType())\n",
    "    imm = imm.withColumn(\"arrdateclean\", adjust_date(imm[\"arrdate\"]))\n",
    "\n",
    "    # clean up the gender column\n",
    "    adjust_gender = F.udf(lambda x: x if x in [\"M\", \"F\"] else \"O\")\n",
    "    imm = imm.withColumn(\"genderclean\", adjust_gender(imm[\"gender\"]))\n",
    "\n",
    "    # bucket the age groups into distinct categories\n",
    "    @F.udf(StringType())\n",
    "    def age_categorizer(age):\n",
    "        \"\"\"\n",
    "        Takes an age value and buckets it into a distinct category\n",
    "        Returns \"Unknown\" if the age is a null value, is negative, or is greater than 120\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "        age: integer or float\n",
    "            the age of a person\n",
    "        \"\"\"\n",
    "        if age == None:\n",
    "            return \"Unknown\"\n",
    "        elif 0 <= age < 18:\n",
    "            return \"Below 18\"\n",
    "        elif 18 <= age < 30:\n",
    "            return \"18 to 29\"\n",
    "        elif 30 <= age < 40:\n",
    "            return \"30 to 39\"\n",
    "        elif 40 <= age < 50:\n",
    "            return \"40 to 49\"\n",
    "        elif 50 <= age < 60:\n",
    "            return \"50 to 59\"\n",
    "        elif 60 <= age <= 120:\n",
    "            return \"60 and Above\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    imm = imm.withColumn(\"agecategory\", age_categorizer(imm[\"i94bir\"]))\n",
    "\n",
    "    # create a unique identifier for traveller profiles\n",
    "    imm = imm.withColumn(\n",
    "        \"profile_id\", \n",
    "        F.lower(F.concat(\n",
    "            imm[\"genderclean\"], \n",
    "            F.substring(imm[\"i94cit_continent\"], 1, 3), \n",
    "            F.substring(imm[\"agecategory\"], 1, 2)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add month and year for partitioning\n",
    "    imm = imm.withColumn(\"month\", F.month(imm[\"arrdateclean\"]))\n",
    "    imm = imm.withColumn(\"year\", F.year(imm[\"arrdateclean\"]))\n",
    "\n",
    "    imm.write.partitionBy(\"month\", \"year\").parquet(preprocessed_output_fp + \"immigration_data\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_main():\n",
    "    \"\"\"\n",
    "    The main function that executes the Spark job that preprocessing the immigration data\n",
    "    \"\"\"\n",
    "    # hardcoded variables\n",
    "    # only for emr cluster\n",
    "    # s3_link_prefix = \"s3://dendcapstoneproject/\"\n",
    "    # file_prefix = \"raw_data/18-83510-I94-Data-2016/i94\"\n",
    "    imm_fps = [\"../data/multiple_sas_files/i94_dec16_sub.sas7bdat\", \"../data/multiple_sas_files/i94_feb16_sub.sas7bdat\"]\n",
    "    mode_labels_fp = \"../data/i94mode_labels.csv\"\n",
    "    port_labels_fp = \"../data/i94port_labels.csv\"\n",
    "    visa_labels_fp = \"../data/i94visa_labels.csv\"\n",
    "    cit_labels_fp = \"../data/i94cit_labels.csv\"\n",
    "    cols_required = [\n",
    "        \"i94cit\", \"i94port\", \"arrdate\",\n",
    "        \"i94mode\", \"i94visa\", \"i94bir\", \"gender\"\n",
    "    ]\n",
    "    preprocessed_output_fp = \"../data/preprocessed_files/\"\n",
    "\n",
    "    # run the proprocessing spark job\n",
    "    spark = initialize_spark()\n",
    "\n",
    "    # only for emr cluster\n",
    "    # imm_fps = get_file_list_from_s3(s3_link_prefix, file_prefix)\n",
    "\n",
    "    # iterate across the SAS files\n",
    "    for imm_fp in imm_fps:\n",
    "\n",
    "        imm = import_immigration_and_add_labels(\n",
    "            spark, imm_fp, mode_labels_fp, port_labels_fp, \n",
    "            visa_labels_fp, cit_labels_fp, cols_required\n",
    "        )\n",
    "\n",
    "        imm = preprocess_immigration_data(imm, preprocessed_output_fp)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run above\n",
    "preprocessing_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact and Dimension Creation Spark Job\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark():\n",
    "    \"\"\"\n",
    "    Initializes a spark instance\n",
    "    \"\"\"\n",
    "    # initialize spark\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"immigration-fact-and-dimension-creation\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_immigration_fact(spark, preprocessed_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Creates the immigration fact table\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    preprocessed_file_path: str\n",
    "        The location of the preprocessed immigration data\n",
    "    output_file_path: str\n",
    "        The location to store the final fact/dimension data\n",
    "    \"\"\"\n",
    "    # read in the immigration data\n",
    "    imm = spark.read.parquet(preprocessed_file_path)\n",
    "\n",
    "    # create the skeleton table that will become fact_immigration table\n",
    "    fact_imm = imm.select(\"i94port_state\", \"profile_id\", \"arrdateclean\").dropDuplicates()\n",
    "\n",
    "    # index columns\n",
    "    index_cols = [\"i94port_state\", \"profile_id\", \"arrdateclean\"]\n",
    "\n",
    "    # add total number of travellers to the above\n",
    "    fact_imm = fact_imm.join(\n",
    "        imm.groupby(index_cols).count(),\n",
    "        on=index_cols,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # add number of travellers by transportation mode\n",
    "    fact_imm = fact_imm.join(\n",
    "        imm.groupby(index_cols).pivot(\"i94mode_label\").count(),\n",
    "        on=index_cols,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # add number of travellers by purpose of visit\n",
    "    fact_imm = fact_imm.join(\n",
    "        imm.groupby(index_cols).pivot(\"i94visa_label\").count(),\n",
    "        on=index_cols,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # drop the not reported column in the fact table\n",
    "    fact_imm = fact_imm.drop(\"Not Reported\")\n",
    "\n",
    "    # rename columns in the fact immigration table\n",
    "    fact_imm_col_names = {\n",
    "        \"i94port_state\": \"state_id\",\n",
    "        \"arrdateclean\": \"arrival_date\",\n",
    "        \"count\": \"all_travellers\",\n",
    "        \"Air\": \"air_travellers\",\n",
    "        \"Land\": \"land_travellers\",\n",
    "        \"Sea\": \"sea_travellers\",\n",
    "        \"Business\": \"business_travellers\",\n",
    "        \"Pleasure\": \"pleasure_travellers\",\n",
    "        \"Student\": \"student_travellers\"\n",
    "    }\n",
    "\n",
    "    for k,v in fact_imm_col_names.items():\n",
    "        fact_imm = fact_imm.withColumnRenamed(k, v)\n",
    "\n",
    "    # fill nulls with 0\n",
    "    fact_imm = fact_imm.fillna(0)\n",
    "\n",
    "    # add a record id column\n",
    "    fact_imm = fact_imm.withColumn(\"record_id\", F.monotonically_increasing_id())\n",
    "\n",
    "    # add a month and year for partitioning\n",
    "    fact_imm = fact_imm.withColumn(\"month\", F.month(fact_imm[\"arrival_date\"]))\n",
    "    fact_imm = fact_imm.withColumn(\"year\", F.year(fact_imm[\"arrival_date\"]))\n",
    "\n",
    "    # write to parquet files\n",
    "    fact_imm.write.partitionBy(\"month\", \"year\").parquet(output_file_path + \"fact_immigration/\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traveller_profile_dimension(spark, preprocessed_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Creates the traveller profile dimension table\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    preprocessed_file_path: str\n",
    "        The location of the preprocessed immigration data\n",
    "    output_file_path: str\n",
    "        The location to store the final fact/dimension data\n",
    "    \"\"\"\n",
    "\n",
    "    # read in the immigration data\n",
    "    imm = spark.read.parquet(preprocessed_file_path)\n",
    "\n",
    "    # create the traveller profile dimension table\n",
    "    dim_traveller_profile = imm.select(\n",
    "        \"profile_id\", \"genderclean\", \"agecategory\", \n",
    "        \"i94cit_continent\", \"i94cit_global_region\"\n",
    "    ).dropDuplicates()\n",
    "\n",
    "    # rename columns\n",
    "    profile_col_names = {\n",
    "        \"genderclean\": \"gender\",\n",
    "        \"agecategory\": \"age_category\",\n",
    "        \"i94cit_continent\": \"citizen_region\",\n",
    "        \"i94cit_global_region\": \"citizen_global_region\"\n",
    "    }\n",
    "\n",
    "    for k,v in profile_col_names.items():\n",
    "        dim_traveller_profile = dim_traveller_profile.withColumnRenamed(k, v)\n",
    "\n",
    "    # write to output file path\n",
    "    dim_traveller_profile.write.parquet(output_file_path + \"dim_traveller_profile/\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_dimension(spark, preprocessed_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Creates the time dimension table\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    preprocessed_file_path: str\n",
    "        The location of the preprocessed immigration data\n",
    "    output_file_path: str\n",
    "        The location to store the final fact/dimension data\n",
    "    \"\"\"\n",
    "    # read in the immigration data\n",
    "    imm = spark.read.parquet(preprocessed_file_path)\n",
    "\n",
    "    # create the time dimension table skeleton\n",
    "    dim_time = imm.select(\"arrdateclean\").drop_duplicates()\n",
    "\n",
    "    # add the date components\n",
    "    dim_time = dim_time.withColumn(\"day\", F.dayofmonth(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"month\", F.month(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"year\", F.year(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"day_of_week\", F.dayofweek(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"month_year\", F.concat(dim_time[\"month\"], F.lit(\"-\"), dim_time[\"year\"]))\n",
    "\n",
    "    # rename the arrdateclean column\n",
    "    dim_time = dim_time.withColumnRenamed(\"arrdateclean\", \"timestamp\")\n",
    "\n",
    "    # write to parquet files\n",
    "    dim_time.write.partitionBy(\"month\", \"year\").parquet(output_file_path + \"dim_time/\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_and_dimension_creation_main():\n",
    "    \"\"\"\n",
    "    The main function that runs the Spark job\n",
    "    to create the fact and dimension tables off the immigration data\n",
    "    \"\"\"\n",
    "    # hardcoded variables\n",
    "    preprocessed_file_path = \"../data/preprocessed_files/immigration_data/\"\n",
    "    output_file_path = \"../data/output_files/\"\n",
    "\n",
    "    # run the spark job\n",
    "    spark = initialize_spark()\n",
    "    create_traveller_profile_dimension(spark, preprocessed_file_path, output_file_path)\n",
    "    create_immigration_fact(spark, preprocessed_file_path, output_file_path)\n",
    "    create_time_dimension(spark, preprocessed_file_path, output_file_path)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run above\n",
    "fact_and_dimension_creation_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"immigration-fact-and-dimension-creation\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data\n",
    "imm_prepoc = spark.read.parquet(\"../data/preprocessed_files/immigration_data/\")\n",
    "fact_imm = spark.read.parquet(\"../data/output_files/fact_immigration\")\n",
    "dim_time = spark.read.parquet(\"../data/output_files/dim_time\")\n",
    "dim_traveller_profile = spark.read.parquet(\"../data/output_files/dim_traveller_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total rows in imm_preproc\n",
    "imm_prepoc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see fact_imm\n",
    "fact_imm.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of travellers (should match total rows above)\n",
    "fact_imm.agg({\"all_travellers\": \"sum\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_time head\n",
    "dim_time.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_traveller_profile head\n",
    "dim_traveller_profile.limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ae5f699e6c33511a9bd2522a947541f4f974526b22d36b2404541bec39e83fe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
