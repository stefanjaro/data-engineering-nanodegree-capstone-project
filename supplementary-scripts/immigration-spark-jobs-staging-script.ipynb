{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration Data Processing\n",
    "\n",
    "The staging script for the Immigration Data Processing Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Spark Job\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark():\n",
    "    \"\"\"\n",
    "    Initializes a spark instance\n",
    "    \"\"\"\n",
    "    # initialize spark\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"immigration-data-preprocessing\")\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:3.0.0-s_2.12\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_immigration_and_add_labels(spark, imm_fp, mode_labels_fp, port_labels_fp, \n",
    "                                      visa_labels_fp, cit_labels_fp, cols_required):\n",
    "    \"\"\"\n",
    "    Imports all of the data required for immigration pre-processing\n",
    "    Filters the immigration data down to only the columns we need\n",
    "    And adds all of the labels to the codified values we're interested in\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    imm_fp: str\n",
    "        The location of the immigration data files\n",
    "    mode_labels_fp: str\n",
    "        The location of the labels file for i94mode\n",
    "    port_labels_fp: str\n",
    "        The location of the labels file for i94port\n",
    "    visa_labels_fp: str\n",
    "        The location of the labels file for i94visa\n",
    "    cit_labels_fp: str\n",
    "        The location of the labels file for i94cit\n",
    "    cols_required: list\n",
    "        The list of columns we require from the immigration data file\n",
    "    \"\"\"\n",
    "    # import the sas files\n",
    "    imm = spark.read.format('com.github.saurfang.sas.spark').load(imm_fp)\n",
    "\n",
    "    # import label files\n",
    "    mode_labels = spark.read.csv(mode_labels_fp, header=True) # for mode of travel\n",
    "    port_labels = spark.read.csv(port_labels_fp, header=True) # for port of entry\n",
    "    visa_labels = spark.read.csv(visa_labels_fp, header=True) # for type of visa\n",
    "    citz_labels = spark.read.csv(cit_labels_fp, header=True) # for citizenship country\n",
    "\n",
    "    # select only the columns we need\n",
    "    imm = imm.select(cols_required)\n",
    "\n",
    "    # add labels to codified values\n",
    "    for df in [mode_labels, port_labels, visa_labels, citz_labels]:\n",
    "        imm = imm.join(\n",
    "            F.broadcast(df),\n",
    "            on=df.columns[0],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    return imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_immigration_data(imm, preprocessed_output_fp):\n",
    "    \"\"\"\n",
    "    Proprocess the immigration data by:\n",
    "        1. Removing records with irrelevant and invalid values\n",
    "        2. Removing records without an arrival date\n",
    "        3. Adjusting the format of the arrival date\n",
    "        4. Adjusting the gender column so everything apart from M and F is O for Other\n",
    "        5. Bucketing the ages into distinct categories\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    imm: spark dataframe\n",
    "        The immigration dataset\n",
    "    preprocessed_output_fp: str\n",
    "        The location to save the preprocessed file\n",
    "    \"\"\"\n",
    "    # hardcoded variables\n",
    "    SAS_START_DATE = datetime.datetime(1960, 1, 1)\n",
    "\n",
    "    # remove the irrelevant ports and invalid citizenship countries from the dataset\n",
    "    imm = imm.where(\n",
    "        (imm[\"i94port_relevant\"] == \"RELEVANT\") &\n",
    "        (imm[\"i94cit_continent\"] != \"INVALID\")\n",
    "    )\n",
    "    \n",
    "    # clean up arrival date which is the number of days after January 1, 1960\n",
    "    imm = imm.where(imm[\"arrdate\"].isNotNull())    \n",
    "    adjust_date = F.udf(lambda x: SAS_START_DATE + datetime.timedelta(days=x), DateType())\n",
    "    imm = imm.withColumn(\"arrdateclean\", adjust_date(imm[\"arrdate\"]))\n",
    "\n",
    "    # clean up the gender column\n",
    "    adjust_gender = F.udf(lambda x: x if x in [\"M\", \"F\"] else \"O\")\n",
    "    imm = imm.withColumn(\"genderclean\", adjust_gender(imm[\"gender\"]))\n",
    "\n",
    "    # bucket the age groups into distinct categories\n",
    "    @F.udf(StringType())\n",
    "    def age_categorizer(age):\n",
    "        \"\"\"\n",
    "        Takes an age value and buckets it into a distinct category\n",
    "        Returns \"Unknown\" if the age is a null value, is negative, or is greater than 120\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "        age: integer or float\n",
    "            the age of a person\n",
    "        \"\"\"\n",
    "        if age == None:\n",
    "            return \"Unknown\"\n",
    "        elif 0 <= age < 18:\n",
    "            return \"Below 18\"\n",
    "        elif 18 <= age < 30:\n",
    "            return \"18 to 29\"\n",
    "        elif 30 <= age < 40:\n",
    "            return \"30 to 39\"\n",
    "        elif 40 <= age < 50:\n",
    "            return \"40 to 49\"\n",
    "        elif 50 <= age < 60:\n",
    "            return \"50 to 59\"\n",
    "        elif 60 <= age <= 120:\n",
    "            return \"60 and Above\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    imm = imm.withColumn(\"agecategory\", age_categorizer(imm[\"i94bir\"]))\n",
    "\n",
    "    # create a unique identifier for traveller profiles\n",
    "    imm = imm.withColumn(\n",
    "        \"profile_id\", \n",
    "        F.lower(F.concat(\n",
    "            imm[\"genderclean\"], \n",
    "            F.substring(imm[\"i94cit_continent\"], 1, 3), \n",
    "            F.substring(imm[\"agecategory\"], 1, 2)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add month and year for partitioning\n",
    "    imm = imm.withColumn(\"month\", F.month(imm[\"arrdateclean\"]))\n",
    "    imm = imm.withColumn(\"year\", F.year(imm[\"arrdateclean\"]))\n",
    "\n",
    "    imm.write.partitionBy(\"month\", \"year\").parquet(preprocessed_output_fp + \"immigration_data\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_main():\n",
    "    \"\"\"\n",
    "    The main function that executes the Spark job that preprocessing the immigration data\n",
    "    \"\"\"\n",
    "    # hardcoded variables\n",
    "    imm_fp = \"../data/i94_feb16_sub.sas7bdat\"\n",
    "    mode_labels_fp = \"../data/i94mode_labels.csv\"\n",
    "    port_labels_fp = \"../data/i94port_labels.csv\"\n",
    "    visa_labels_fp = \"../data/i94visa_labels.csv\"\n",
    "    cit_labels_fp = \"../data/i94cit_labels.csv\"\n",
    "    cols_required = [\n",
    "        \"i94cit\", \"i94port\", \"arrdate\",\n",
    "        \"i94mode\", \"i94visa\", \"i94bir\", \"gender\"\n",
    "    ]\n",
    "    preprocessed_output_fp = \"../data/preprocessed_files/\"\n",
    "\n",
    "    # run the proprocessing spark job\n",
    "    spark = initialize_spark()\n",
    "\n",
    "    imm = import_immigration_and_add_labels(\n",
    "        spark, imm_fp, mode_labels_fp, port_labels_fp, \n",
    "        visa_labels_fp, cit_labels_fp, cols_required\n",
    "    )\n",
    "\n",
    "    imm = preprocess_immigration_data(imm, preprocessed_output_fp)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 00:35:17 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.9 instead (on interface wlo1)\n",
      "22/04/07 00:35:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /home/stefanjaro/.ivy2/cache\n",
      "The jars for the packages stored in: /home/stefanjaro/.ivy2/jars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/stefanjaro/miniconda3/envs/analytics/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3764784-e066-4025-8148-e5e2ee335425;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;3.0.0-s_2.12 in spark-packages\n",
      "\tfound com.epam#parso;2.0.11 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.12;12.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.13.2 in central\n",
      ":: resolution report :: resolve 188ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.11 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.13.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.12;12.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;3.0.0-s_2.12 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e3764784-e066-4025-8148-e5e2ee335425\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/6ms)\n",
      "22/04/07 00:35:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/07 00:35:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/07 00:35:19 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/04/07 00:35:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/04/07 00:35:33 WARN MemoryManager: Total allocation exceeds 95.00% (921,436,148 bytes) of heap memory\n",
      "Scaling row group sizes to 98.07% for 7 writers\n",
      "22/04/07 00:35:33 WARN MemoryManager: Total allocation exceeds 95.00% (921,436,148 bytes) of heap memory\n",
      "Scaling row group sizes to 85.82% for 8 writers\n",
      "22/04/07 00:35:35 WARN MemoryManager: Total allocation exceeds 95.00% (921,436,148 bytes) of heap memory\n",
      "Scaling row group sizes to 98.07% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run above\n",
    "preprocessing_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact and Dimension Creation Spark Job\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spark():\n",
    "    \"\"\"\n",
    "    Initializes a spark instance\n",
    "    \"\"\"\n",
    "    # initialize spark\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"immigration-fact-and-dimension-creation\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_immigration_fact(spark, preprocessed_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Creates the immigration fact table\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    preprocessed_file_path: str\n",
    "        The location of the preprocessed immigration data\n",
    "    output_file_path: str\n",
    "        The location to store the final fact/dimension data\n",
    "    \"\"\"\n",
    "    # read in the immigration data\n",
    "    imm = spark.read.parquet(preprocessed_file_path)\n",
    "\n",
    "    # create the skeleton table that will become fact_immigration table\n",
    "    fact_imm = imm.select(\"i94port_state\", \"profile_id\", \"arrdateclean\").dropDuplicates()\n",
    "\n",
    "    # index columns\n",
    "    index_cols = [\"i94port_state\", \"profile_id\", \"arrdateclean\"]\n",
    "\n",
    "    # add total number of travellers to the above\n",
    "    fact_imm = fact_imm.join(\n",
    "        imm.groupby(index_cols).count(),\n",
    "        on=index_cols,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # add number of travellers by transportation mode\n",
    "    fact_imm = fact_imm.join(\n",
    "        imm.groupby(index_cols).pivot(\"i94mode_label\").count(),\n",
    "        on=index_cols,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # add number of travellers by purpose of visit\n",
    "    fact_imm = fact_imm.join(\n",
    "        imm.groupby(index_cols).pivot(\"i94visa_label\").count(),\n",
    "        on=index_cols,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # drop the not reported column in the fact table\n",
    "    fact_imm = fact_imm.drop(\"Not Reported\")\n",
    "\n",
    "    # rename columns in the fact immigration table\n",
    "    fact_imm_col_names = {\n",
    "        \"i94port_state\": \"state_id\",\n",
    "        \"arrdateclean\": \"arrival_date\",\n",
    "        \"count\": \"all_travellers\",\n",
    "        \"Air\": \"air_travellers\",\n",
    "        \"Land\": \"land_travellers\",\n",
    "        \"Sea\": \"sea_travellers\",\n",
    "        \"Business\": \"business_travellers\",\n",
    "        \"Pleasure\": \"pleasure_travellers\",\n",
    "        \"Student\": \"student_travellers\"\n",
    "    }\n",
    "\n",
    "    for k,v in fact_imm_col_names.items():\n",
    "        fact_imm = fact_imm.withColumnRenamed(k, v)\n",
    "\n",
    "    # fill nulls with 0\n",
    "    fact_imm = fact_imm.fillna(0)\n",
    "\n",
    "    # add a record id column\n",
    "    fact_imm = fact_imm.withColumn(\"record_id\", F.monotonically_increasing_id())\n",
    "\n",
    "    # add a month and year for partitioning\n",
    "    fact_imm = fact_imm.withColumn(\"month\", F.month(fact_imm[\"arrival_date\"]))\n",
    "    fact_imm = fact_imm.withColumn(\"year\", F.year(fact_imm[\"arrival_date\"]))\n",
    "\n",
    "    # write to parquet files\n",
    "    fact_imm.write.partitionBy(\"month\", \"year\").parquet(output_file_path + \"fact_immigration/\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traveller_profile_dimension(spark, preprocessed_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Creates the traveller profile dimension table\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    preprocessed_file_path: str\n",
    "        The location of the preprocessed immigration data\n",
    "    output_file_path: str\n",
    "        The location to store the final fact/dimension data\n",
    "    \"\"\"\n",
    "\n",
    "    # read in the immigration data\n",
    "    imm = spark.read.parquet(preprocessed_file_path)\n",
    "\n",
    "    # create the traveller profile dimension table\n",
    "    dim_traveller_profile = imm.select(\n",
    "        \"profile_id\", \"genderclean\", \"agecategory\", \n",
    "        \"i94cit_continent\", \"i94cit_global_region\"\n",
    "    ).dropDuplicates()\n",
    "\n",
    "    # rename columns\n",
    "    profile_col_names = {\n",
    "        \"genderclean\": \"gender\",\n",
    "        \"agecategory\": \"age_category\",\n",
    "        \"i94cit_continent\": \"citizen_region\",\n",
    "        \"i94cit_global_region\": \"citizen_global_region\"\n",
    "    }\n",
    "\n",
    "    for k,v in profile_col_names.items():\n",
    "        dim_traveller_profile = dim_traveller_profile.withColumnRenamed(k, v)\n",
    "\n",
    "    # write to output file path\n",
    "    dim_traveller_profile.write.parquet(output_file_path + \"dim_traveller_profile/\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_dimension(spark, preprocessed_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Creates the time dimension table\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    spark: spark instance\n",
    "        A spark instance that's been initialized\n",
    "    preprocessed_file_path: str\n",
    "        The location of the preprocessed immigration data\n",
    "    output_file_path: str\n",
    "        The location to store the final fact/dimension data\n",
    "    \"\"\"\n",
    "    # read in the immigration data\n",
    "    imm = spark.read.parquet(preprocessed_file_path)\n",
    "\n",
    "    # create the time dimension table skeleton\n",
    "    dim_time = imm.select(\"arrdateclean\").drop_duplicates()\n",
    "\n",
    "    # add the date components\n",
    "    dim_time = dim_time.withColumn(\"day\", F.dayofmonth(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"month\", F.month(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"year\", F.year(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"day_of_week\", F.dayofweek(dim_time[\"arrdateclean\"]))\n",
    "    dim_time = dim_time.withColumn(\"month_year\", F.concat(dim_time[\"month\"], F.lit(\"-\"), dim_time[\"year\"]))\n",
    "\n",
    "    # rename the arrdateclean column\n",
    "    dim_time = dim_time.withColumnRenamed(\"arrdateclean\", \"timestamp\")\n",
    "\n",
    "    # write to parquet files\n",
    "    dim_time.write.partitionBy(\"month\", \"year\").parquet(output_file_path + \"dim_time/\", \"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_and_dimension_creation_main():\n",
    "    \"\"\"\n",
    "    The main function that runs the Spark job\n",
    "    to create the fact and dimension tables off the immigration data\n",
    "    \"\"\"\n",
    "    # hardcoded variables\n",
    "    preprocessed_file_path = \"../data/preprocessed_files/immigration_data/\"\n",
    "    output_file_path = \"../data/output_files/\"\n",
    "\n",
    "    # run the spark job\n",
    "    spark = initialize_spark()\n",
    "    create_traveller_profile_dimension(spark, preprocessed_file_path, output_file_path)\n",
    "    create_immigration_fact(spark, preprocessed_file_path, output_file_path)\n",
    "    create_time_dimension(spark, preprocessed_file_path, output_file_path)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run above\n",
    "fact_and_dimension_creation_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"immigration-fact-and-dimension-creation\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data\n",
    "imm_prepoc = spark.read.parquet(\"../data/preprocessed_files/immigration_data/\")\n",
    "fact_imm = spark.read.parquet(\"../data/output_files/fact_immigration\")\n",
    "dim_time = spark.read.parquet(\"../data/output_files/dim_time\")\n",
    "dim_traveller_profile = spark.read.parquet(\"../data/output_files/dim_traveller_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2158058"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total rows in imm_preproc\n",
    "imm_prepoc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>profile_id</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>all_travellers</th>\n",
       "      <th>air_travellers</th>\n",
       "      <th>land_travellers</th>\n",
       "      <th>sea_travellers</th>\n",
       "      <th>business_travellers</th>\n",
       "      <th>pleasure_travellers</th>\n",
       "      <th>student_travellers</th>\n",
       "      <th>record_id</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL</td>\n",
       "      <td>meur18</td>\n",
       "      <td>2016-02-20</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NJ</td>\n",
       "      <td>meur40</td>\n",
       "      <td>2016-02-13</td>\n",
       "      <td>375</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL</td>\n",
       "      <td>meur18</td>\n",
       "      <td>2016-02-19</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GA</td>\n",
       "      <td>feur50</td>\n",
       "      <td>2016-02-19</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FL</td>\n",
       "      <td>feur50</td>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>390</td>\n",
       "      <td>386</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_id profile_id arrival_date  all_travellers  air_travellers  \\\n",
       "0       IL     meur18   2016-02-20             121             121   \n",
       "1       NJ     meur40   2016-02-13             375             375   \n",
       "2       IL     meur18   2016-02-19              83              83   \n",
       "3       GA     feur50   2016-02-19              51              51   \n",
       "4       FL     feur50   2016-02-11             390             386   \n",
       "\n",
       "   land_travellers  sea_travellers  business_travellers  pleasure_travellers  \\\n",
       "0                0               0                   38                   83   \n",
       "1                0               0                   59                  315   \n",
       "2                0               0                   15                   68   \n",
       "3                0               0                    4                   47   \n",
       "4                0               4                   13                  377   \n",
       "\n",
       "   student_travellers  record_id  month  year  \n",
       "0                   0          0      2  2016  \n",
       "1                   1          1      2  2016  \n",
       "2                   0          2      2  2016  \n",
       "3                   0          3      2  2016  \n",
       "4                   0          4      2  2016  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see fact_imm\n",
    "fact_imm.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sum(all_travellers)=2158058)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of travellers (should match total rows above)\n",
    "fact_imm.agg({\"all_travellers\": \"sum\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month_year</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-04</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-08</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-03</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-22</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-02-09</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-02-18</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-02-12</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-02-19</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2-2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  day  day_of_week month_year  month  year\n",
       "0  2016-02-04    4            5     2-2016      2  2016\n",
       "1  2016-02-08    8            2     2-2016      2  2016\n",
       "2  2016-02-03    3            4     2-2016      2  2016\n",
       "3  2016-02-22   22            2     2-2016      2  2016\n",
       "4  2016-02-11   11            5     2-2016      2  2016\n",
       "5  2016-02-09    9            3     2-2016      2  2016\n",
       "6  2016-02-15   15            2     2-2016      2  2016\n",
       "7  2016-02-18   18            5     2-2016      2  2016\n",
       "8  2016-02-12   12            6     2-2016      2  2016\n",
       "9  2016-02-19   19            6     2-2016      2  2016"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim_time head\n",
    "dim_time.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_category</th>\n",
       "      <th>citizen_region</th>\n",
       "      <th>citizen_global_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnorbe</td>\n",
       "      <td>M</td>\n",
       "      <td>Below 18</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fsou60</td>\n",
       "      <td>F</td>\n",
       "      <td>60 and Above</td>\n",
       "      <td>SOUTH AMERICA</td>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oasibe</td>\n",
       "      <td>O</td>\n",
       "      <td>Below 18</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meur40</td>\n",
       "      <td>M</td>\n",
       "      <td>40 to 49</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>NORTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fafr40</td>\n",
       "      <td>F</td>\n",
       "      <td>40 to 49</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oasi60</td>\n",
       "      <td>O</td>\n",
       "      <td>60 and Above</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>NORTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>foce50</td>\n",
       "      <td>F</td>\n",
       "      <td>50 to 59</td>\n",
       "      <td>OCEANIA</td>\n",
       "      <td>NORTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>foce60</td>\n",
       "      <td>F</td>\n",
       "      <td>60 and Above</td>\n",
       "      <td>OCEANIA</td>\n",
       "      <td>NORTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feur50</td>\n",
       "      <td>F</td>\n",
       "      <td>50 to 59</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>NORTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>oasi30</td>\n",
       "      <td>O</td>\n",
       "      <td>30 to 39</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_id gender  age_category citizen_region citizen_global_region\n",
       "0     mnorbe      M      Below 18  NORTH AMERICA                 SOUTH\n",
       "1     fsou60      F  60 and Above  SOUTH AMERICA                 SOUTH\n",
       "2     oasibe      O      Below 18           ASIA                 SOUTH\n",
       "3     meur40      M      40 to 49         EUROPE                 NORTH\n",
       "4     fafr40      F      40 to 49         AFRICA                 SOUTH\n",
       "5     oasi60      O  60 and Above           ASIA                 NORTH\n",
       "6     foce50      F      50 to 59        OCEANIA                 NORTH\n",
       "7     foce60      F  60 and Above        OCEANIA                 NORTH\n",
       "8     feur50      F      50 to 59         EUROPE                 NORTH\n",
       "9     oasi30      O      30 to 39           ASIA                 SOUTH"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim_traveller_profile head\n",
    "dim_traveller_profile.limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ae5f699e6c33511a9bd2522a947541f4f974526b22d36b2404541bec39e83fe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
